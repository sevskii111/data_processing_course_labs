{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7892a287",
   "metadata": {},
   "source": [
    "- Прочитайте главы 12-14 из книги [Spark: The Definitive Guide](https://analyticsdata24.files.wordpress.com/2020/02/spark-the-definitive-guide40www.bigdatabugs.com_.pdf#%5B%7B%22num%22%3A484%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C792%2Cnull%5D).\n",
    "- Прочитайте главу 2 из книги [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf#%5B%7B%22num%22%3A348%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2Cnull%2C588.58502%2Cnull%5D).\n",
    "- Выполните задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab4\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "textFiles = sc.textFile('hdfs://localhost:9000//user/student/gutenberg/*')\n",
    "\n",
    "textFiles.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5d53c",
   "metadata": {},
   "source": [
    "№0: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/WordCount\" и изучите его содержимое.\n",
    "\n",
    "Здесь представлена реализация подсчета слов в текстовых документах по парадигме MapReduce для Apache Hadoop. В папке Input находятся текстовые файлы с входными данными. Они же загружены в файловую систему HDFS по адресу hdfs://localhost:9000//user/student/gutenberg. В папке Src представлен исходный код стадий map и reduce. В папке Scripts есть два скрипта: cluster_run.sh и local_run.sh. Первый выполняет подсчет слов на платформе Hadoop. Второй выполняет ту же задачу без Hadoop. В папку Output попадают результаты выполнения скриптов.\n",
    "\n",
    "Код ниже подсчитывает количество вхождений слов в текстовые документы с помощью платформы Apache Spark. Сравните этот код с реализацией той же задачи для Apache Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c320f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = textFiles\\\n",
    "    .flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "counts = words\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effa31e",
   "metadata": {},
   "source": [
    "№1: Найдите количество вхождений только тех слов, длина которых не менее 5 символов. Выведите их в порядке убывания.\n",
    "\n",
    "[('which', 3637),\n",
    " ('their', 1691),\n",
    " ('there', 1224),\n",
    " ('other', 954),\n",
    " ('would', 884),\n",
    " ('these', 846)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398f03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7642de69",
   "metadata": {},
   "source": [
    "№2: Найдите количество повторений последовательностей из двух слов в текстах. Выведите их в порядке убывания.\n",
    "\n",
    "[(('of', 'the'), 6900),\n",
    " (('in', 'the'), 3501),\n",
    " (('to', 'the'), 2208),\n",
    " (('and', 'the'), 1637),\n",
    " (('on', 'the'), 1618),\n",
    " (('from', 'the'), 1231)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = textFiles.flatMap(lambda line: line.split(\".\"))\\\n",
    "   .map(lambda line: line.strip().split(\" \"))\\\n",
    "   .flatMap(lambda xs: (tuple(x) for x in zip(xs, xs[1: ])))\n",
    "\n",
    "pair_counts = bigrams\\\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fa531",
   "metadata": {},
   "source": [
    "№3: Найдите самое длинное слово из тех, которые состоят только из буквенно-цифровых символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123af60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037664dc",
   "metadata": {},
   "source": [
    "№4: Для каждой буквы найдите количество уникальных слов в текстах, которые на нее начинаются. Приведите буквы к верхнему регистру, отсортируйте их в алфавитном порядке.\n",
    "\n",
    "[('A', 4268), ('B', 4460), ('C', 6660), ('D', 4156), ('E', 2856), ('F', 3482)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = words\\\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece9dcb",
   "metadata": {},
   "source": [
    "№ 5: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/Weather\" и изучите его содержимое. Здесь представлен код MapReduce для Apache Hadoop, который обрабатывает погодные данные [National Climatic Data Center](http://www.ncdc.noaa.gov/). Подробности про задачу, которую решает данная программа, прочитайте во второй главе [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf).\n",
    "\n",
    "Погодные данные загружены в файловую систему HDFS по адресу hdfs://localhost:9000/user/student/weather. Там присутствуют данные только за 2022 год. Реализуйте решение той же задачи с погодными данными в Apache Spark с помощью методов flatMap и reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30759d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "weatherFiles = sc.wholeTextFiles('hdfs://localhost:9000/user/student/weather/*')\n",
    "\n",
    "def mapper(text):\n",
    "    ...\n",
    "\n",
    "def reducer(a, b):\n",
    "    ...\n",
    "    \n",
    "weatherFiles\\\n",
    "    .flatMap(mapper)\\\n",
    "    .reduceByKey(reducer)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323ca5b",
   "metadata": {},
   "source": [
    "№6: Решите задачу из задания 5 с помощью SQL и методов DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "weatherLines = sc.textFile('hdfs://localhost:9000/user/student/weather/*')\n",
    "weatherColumns = [\"year\",\"temperature\", \"quality\"]\n",
    "weatherDF = weatherLines\\\n",
    "    .map(lambda val: (int(val[15:19]), int(val[87:92]), int(val[92:93])))\\\n",
    "    .toDF(weatherColumns)\n",
    "\n",
    "weatherDF.createOrReplaceTempView(\"Weather\")\n",
    "\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = weatherDF\\\n",
    "    ...\n",
    "\n",
    "print(sqlWay.rdd.map(lambda x: (x[0], x[1])).collect())\n",
    "print(dataFrameWay.rdd.map(lambda x: (x[0], x[1])).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
