{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7892a287",
   "metadata": {},
   "source": [
    "- Прочитайте главы 12-14 из книги [Spark: The Definitive Guide](https://analyticsdata24.files.wordpress.com/2020/02/spark-the-definitive-guide40www.bigdatabugs.com_.pdf#%5B%7B%22num%22%3A484%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C792%2Cnull%5D).\n",
    "- Прочитайте главу 2 из книги [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf#%5B%7B%22num%22%3A348%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2Cnull%2C588.58502%2Cnull%5D).\n",
    "- Выполните задания.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fa85fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/29 00:53:20 WARN Utils: Your hostname, student-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/12/29 00:53:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/29 00:53:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/29 00:53:26 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:191)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:314)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Ulysses, by James Joyce'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Lab4\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "textFiles = sc.textFile(\"hdfs://localhost:9000//user/student/gutenberg/*\")\n",
    "\n",
    "textFiles.first()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5e5d53c",
   "metadata": {},
   "source": [
    "№0: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/WordCount\" и изучите его содержимое.\n",
    "\n",
    "Здесь представлена реализация подсчета слов в текстовых документах по парадигме MapReduce для Apache Hadoop. В папке Input находятся текстовые файлы с входными данными. Они же загружены в файловую систему HDFS по адресу hdfs://localhost:9000//user/student/gutenberg. В папке Src представлен исходный код стадий map и reduce. В папке Scripts есть два скрипта: cluster_run.sh и local_run.sh. Первый выполняет подсчет слов на платформе Hadoop. Второй выполняет ту же задачу без Hadoop. В папку Output попадают результаты выполнения скриптов.\n",
    "\n",
    "Код ниже подсчитывает количество вхождений слов в текстовые документы с помощью платформы Apache Spark. Сравните этот код с реализацией той же задачи для Apache Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c320f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Project', 185),\n",
       " ('of', 23950),\n",
       " ('Ulysses,', 2),\n",
       " ('James', 30),\n",
       " ('Joyce', 3),\n",
       " ('', 34559)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts.take(6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2effa31e",
   "metadata": {},
   "source": [
    "№1: Найдите количество вхождений только тех слов, длина которых не менее 5 символов. Выведите их в порядке убывания.\n",
    "\n",
    "[('which', 3637),\n",
    "('their', 1691),\n",
    "('there', 1224),\n",
    "('other', 954),\n",
    "('would', 884),\n",
    "('these', 846)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a398f03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('which', 3637),\n",
       " ('their', 1691),\n",
       " ('there', 1224),\n",
       " ('other', 954),\n",
       " ('would', 884),\n",
       " ('these', 846)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_5_or_longer = (\n",
    "    words.filter(lambda word: len(word) >= 5)\n",
    "    .map(lambda word: (word, 1))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .sortBy(lambda x: x[1], False)\n",
    ")\n",
    "\n",
    "counts_5_or_longer.take(6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7642de69",
   "metadata": {},
   "source": [
    "№2: Найдите количество повторений последовательностей из двух слов в текстах. Выведите их в порядке убывания.\n",
    "\n",
    "[(('of', 'the'), 6900),\n",
    "(('in', 'the'), 3501),\n",
    "(('to', 'the'), 2208),\n",
    "(('and', 'the'), 1637),\n",
    "(('on', 'the'), 1618),\n",
    "(('from', 'the'), 1231)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a2eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 6900),\n",
       " (('in', 'the'), 3501),\n",
       " (('to', 'the'), 2208),\n",
       " (('and', 'the'), 1637),\n",
       " (('on', 'the'), 1618),\n",
       " (('from', 'the'), 1231)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = (\n",
    "    textFiles.flatMap(lambda line: line.split(\".\"))\n",
    "    .map(lambda line: line.strip().split(\" \"))\n",
    "    .map(lambda line_arr: [word for word in line_arr if word])\n",
    "    .flatMap(lambda xs: (tuple(x) for x in zip(xs, xs[1:])))\n",
    ")\n",
    "\n",
    "pair_counts = (\n",
    "    bigrams.map(lambda bigram: (bigram, 1))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .sortBy(lambda x: x[1], False)\n",
    ")\n",
    "\n",
    "pair_counts.take(6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "224fa531",
   "metadata": {},
   "source": [
    "№3: Найдите самое длинное слово из тех, которые состоят только из буквенно-цифровых символов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6123af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalgymnasiummuseumsanatoriumandsuspensoriumsordinaryprivatdocentge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "words_with_lens = (\n",
    "    words.filter(lambda word: word.isalnum())\n",
    "    .map(lambda word: (len(word), word))\n",
    "    .sortByKey(False)\n",
    ")\n",
    "\n",
    "print(words_with_lens.take(1)[0][1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "037664dc",
   "metadata": {},
   "source": [
    "№4: Для каждой буквы найдите количество уникальных слов в текстах, которые на нее начинаются. Приведите буквы к верхнему регистру, отсортируйте их в алфавитном порядке.\n",
    "\n",
    "[('A', 4268), ('B', 4460), ('C', 6660), ('D', 4156), ('E', 2856), ('F', 3482)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2723bdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 4268), ('B', 4460), ('C', 6660), ('D', 4156), ('E', 2856), ('F', 3482)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counts = (\n",
    "    words.filter(lambda word: len(word) > 0)\n",
    "    .distinct()\n",
    "    .flatMap(lambda word: word[0])\n",
    "    .filter(lambda c: c.isalpha())\n",
    "    .map(lambda c: c.capitalize())\n",
    "    .map(lambda c: (c, 1))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .sortByKey()\n",
    ")\n",
    "\n",
    "char_counts.take(6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ece9dcb",
   "metadata": {},
   "source": [
    "№ 5: Откройте каталог \"/home/student/Work/Labs/3.Hadoop/Weather\" и изучите его содержимое. Здесь представлен код MapReduce для Apache Hadoop, который обрабатывает погодные данные [National Climatic Data Center](http://www.ncdc.noaa.gov/). Подробности про задачу, которую решает данная программа, прочитайте во второй главе [Hadoop: The Definitive Guide](https://grut-computing.com/HadoopBook.pdf).\n",
    "\n",
    "Погодные данные загружены в файловую систему HDFS по адресу hdfs://localhost:9000/user/student/weather. Там присутствуют данные только за 2022 год. Реализуйте решение той же задачи с погодными данными в Apache Spark с помощью методов flatMap и reduceByKey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30759d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2022', 320)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "weatherFiles = sc.wholeTextFiles(\"hdfs://localhost:9000/user/student/weather/*\")\n",
    "\n",
    "\n",
    "def mapper(text):\n",
    "    lines = text[1].split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        (year, temp, q) = (line[15:19], line[87:92], line[92:93])\n",
    "        if temp != \"+9999\" and re.match(\"[01459]\", q):\n",
    "            yield year, int(temp)\n",
    "\n",
    "\n",
    "def reducer(a, b):\n",
    "    return max(a, b)\n",
    "\n",
    "\n",
    "weatherFiles.flatMap(mapper).reduceByKey(reducer).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1323ca5b",
   "metadata": {},
   "source": [
    "№6: Решите задачу из задания 5 с помощью SQL и методов DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04d0227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2022, 320)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:=================================================>      (57 + 6) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2022, 320)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "weatherLines = sc.textFile(\"hdfs://localhost:9000/user/student/weather/*\")\n",
    "weatherColumns = [\"year\", \"temperature\", \"quality\"]\n",
    "weatherDF = weatherLines.map(\n",
    "    lambda val: (int(val[15:19]), int(val[87:92]), int(val[92:93]))\n",
    ").toDF(weatherColumns)\n",
    "\n",
    "weatherDF.createOrReplaceTempView(\"Weather\")\n",
    "\n",
    "sqlWay = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT year, max(temperature)\n",
    "    FROM Weather\n",
    "    WHERE quality IN (0,1,4,5,9) AND temperature != 9999\n",
    "    GROUP BY year\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "dataFrameWay = (\n",
    "    weatherDF.where(\n",
    "        weatherDF.quality.isin([0, 1, 4, 5, 9]) & (weatherDF.temperature != 9999)\n",
    "    )\n",
    "    .groupBy(weatherDF.year)\n",
    "    .agg(max(weatherDF.temperature))\n",
    ")\n",
    "\n",
    "print(sqlWay.rdd.map(lambda x: (x[0], x[1])).collect())\n",
    "print(dataFrameWay.rdd.map(lambda x: (x[0], x[1])).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d18f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:24:02) \n[Clang 11.1.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
